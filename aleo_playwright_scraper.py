"""
Aleo.com Scraper s Playwright podporou pro Angular SPA
Playwright m√° lep≈°√≠ podporu pro Single Page Applications ne≈æ Selenium
"""

import asyncio
import re
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeout
import pandas as pd
from datetime import datetime
import os
import argparse
import time

# === KONFIGURACE ===
OUTPUT_DIR = "output"
MAX_CLOUDFLARE_WAIT = 120  # sekund na vy≈ôe≈°en√≠ Cloudflare
PAGE_LOAD_TIMEOUT = 30000  # 30s timeout pro naƒçten√≠ str√°nky
NETWORK_IDLE_TIMEOUT = 10000  # 10s pro network idle

# === HELPER FUNKCE ===
def create_output_dir():
    """Vytvo≈ô√≠ v√Ωstupn√≠ adres√°≈ô, pokud neexistuje"""
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
        print(f"üìÅ Vytvo≈ôen adres√°≈ô: {OUTPUT_DIR}")

async def wait_for_cloudflare(page):
    """
    ƒåek√° na vy≈ôe≈°en√≠ Cloudflare challenge
    """
    print("\n‚ö†Ô∏è  CLOUDFLARE DETEKOV√ÅNA!")
    print("="*60)
    print("MANU√ÅLN√ç KROK:")
    print("1. Kliknƒõte na checkbox 'Verify you are human'")
    print("2. Vy≈ôe≈°te p≈ô√≠padnou captcha")
    print("3. Poƒçkejte, a≈æ se str√°nka naƒçte")
    print(f"\nƒåek√°m max. {MAX_CLOUDFLARE_WAIT} sekund...")
    print("="*60)
    
    start_time = time.time()
    
    while time.time() - start_time < MAX_CLOUDFLARE_WAIT:
        content = await page.content()
        
        # Kontrola, zda Cloudflare zmizela
        if "cloudflare" not in content.lower() or "challenge" not in content.lower():
            print("‚úÖ Cloudflare vy≈ôe≈°ena!")
            return True
        
        await asyncio.sleep(1)
    
    print("‚ùå Timeout - Cloudflare nebyla vy≈ôe≈°ena vƒças")
    return False

async def scrape_category(page, category_url, max_companies=10):
    """
    Scrapuje jednu kategorii pomoc√≠ Playwright
    """
    companies = []
    
    print(f"\nüìÇ Kategorie: {category_url}")
    
    try:
        # Naƒçten√≠ str√°nky
        await page.goto(category_url, wait_until='domcontentloaded', timeout=PAGE_LOAD_TIMEOUT)
        
        # Kontrola Cloudflare
        content = await page.content()
        if "cloudflare" in content.lower() or "challenge" in content.lower():
            if not await wait_for_cloudflare(page):
                return companies
        
        # ƒåek√°n√≠ na naƒçten√≠ Angular obsahu - KL√çƒåOV√â PRO SPA!
        print("  ‚è≥ ƒåek√°m na naƒçten√≠ Angular obsahu...")
        
        try:
            # ƒåek√°me na network idle - signalizuje, ≈æe API requesty skonƒçily
            await page.wait_for_load_state('networkidle', timeout=NETWORK_IDLE_TIMEOUT)
            print("  ‚úÖ Network idle - API po≈æadavky dokonƒçeny")
        except PlaywrightTimeout:
            print("  ‚ö†Ô∏è  Network idle timeout - pokraƒçuji...")
        
        # ƒåek√°me na specifick√Ω Angular element s firmami
        try:
            await page.wait_for_selector('.catalog-row-container', timeout=10000)
            print("  ‚úÖ Angular komponenta naƒçtena")
        except PlaywrightTimeout:
            print("  ‚ö†Ô∏è  Angular komponenta nenalezena - mo≈æn√° ≈æ√°dn√© firmy")
        
        # Scrollov√°n√≠ pro naƒçten√≠ lazy-loaded obsahu
        print("  üìú Scrolluji str√°nku...")
        for i in range(3):
            await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
            await asyncio.sleep(1)
        
        # Z√≠sk√°n√≠ aktu√°ln√≠ho HTML po naƒçten√≠ Angularu
        content = await page.content()
        
        # Debug: ulo≈æen√≠ HTML
        debug_file = "debug_playwright.html"
        with open(debug_file, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"  üíæ Debug HTML ulo≈æeno: {debug_file}")
        
        # Hled√°n√≠ odkaz≈Ø na firmy pomoc√≠ Playwright lok√°tor≈Ø
        # Zkus√≠me naj√≠t linky s pattern /pl/firma/
        links = await page.locator('a[href*="/pl/firma/"]').all()
        
        print(f"  ‚úÖ Nalezeno {len(links)} odkaz≈Ø na firmy")
        
        # Zpracov√°n√≠ firem
        processed = 0
        seen_urls = set()
        
        for link in links:
            if processed >= max_companies:
                break
            
            try:
                href = await link.get_attribute('href')
                if not href or href in seen_urls:
                    continue
                
                # P≈ôeskoƒçit ne≈æ√°douc√≠ linky
                if any(skip in href.lower() for skip in ['linkedin', 'facebook', 'cookie', 'privacy', '/pl/firmy/']):
                    continue
                
                seen_urls.add(href)
                
                # Kompletn√≠ URL
                if href.startswith('/'):
                    full_url = f"https://aleo.com{href}"
                else:
                    full_url = href
                
                processed += 1
                print(f"\n  [{processed}/{max_companies}] Zpracov√°v√°m: {full_url}")
                
                # Z√≠sk√°n√≠ n√°zvu firmy z textu linku nebo z URL
                try:
                    company_name = await link.text_content()
                    if not company_name or len(company_name.strip()) == 0:
                        # Extrahuj n√°zev z URL
                        company_name = href.split('/')[-1].replace('-', ' ').title()
                except:
                    company_name = href.split('/')[-1].replace('-', ' ').title()
                
                print(f"    üè¢ {company_name}")
                
                companies.append({
                    'name': company_name.strip(),
                    'profile_url': full_url,
                    'website': '',
                    'email': '',
                    'category': category_url
                })
                
            except Exception as e:
                print(f"    ‚ö†Ô∏è  Chyba p≈ôi zpracov√°n√≠ linku: {str(e)}")
                continue
        
        print(f"\n‚úÖ Z kategorie z√≠sk√°no: {len(companies)} firem")
        
    except Exception as e:
        print(f"‚ùå Chyba p≈ôi scrapov√°n√≠ kategorie: {str(e)}")
    
    return companies

async def main(categories, max_companies_per_category):
    """
    Hlavn√≠ funkce scraperu
    """
    print("\n" + "="*60)
    print("üöÄ ALEO.COM SCRAPER S PLAYWRIGHT")
    print("="*60)
    
    all_companies = []
    
    async with async_playwright() as p:
        # Spu≈°tƒõn√≠ browseru (headless=False pro Cloudflare)
        browser = await p.chromium.launch(headless=False)
        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36'
        )
        page = await context.new_page()
        
        print(f"\n‚úÖ Budu prohled√°vat {len(categories)} kategori√≠:")
        for cat in categories:
            print(f"  ‚Ä¢ {cat}")
        
        print(f"\n‚öôÔ∏è  Limit: {max_companies_per_category} firem z ka≈æd√© kategorie")
        
        # Proch√°zen√≠ kategori√≠
        for idx, category_url in enumerate(categories, 1):
            print(f"\n{'='*60}")
            print(f"KATEGORIE {idx}/{len(categories)}")
            print("="*60)
            
            companies = await scrape_category(page, category_url, max_companies_per_category)
            all_companies.extend(companies)
            
            # Pauza mezi kategoriemi
            if idx < len(categories):
                print(f"\n‚è∏Ô∏è  Pauza 3s p≈ôed dal≈°√≠ kategori√≠...")
                await asyncio.sleep(3)
        
        await browser.close()
    
    # Export dat
    if all_companies:
        create_output_dir()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # CSV
        csv_file = os.path.join(OUTPUT_DIR, f"aleo_playwright_{timestamp}.csv")
        df = pd.DataFrame(all_companies)
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        
        # Excel
        xlsx_file = os.path.join(OUTPUT_DIR, f"aleo_playwright_{timestamp}.xlsx")
        df.to_excel(xlsx_file, index=False, engine='openpyxl')
        
        print("\n" + "="*60)
        print("‚úÖ HOTOVO!")
        print("="*60)
        print(f"Celkem nalezeno firem: {len(all_companies)}")
        print(f"Firem s emailem: {sum(1 for c in all_companies if c['email'])}")
        print(f"\nüìÅ Soubory:")
        print(f"  ‚Ä¢ {csv_file}")
        print(f"  ‚Ä¢ {xlsx_file}")
    else:
        print("\n‚ùå Nebyly nalezeny ≈æ√°dn√© firmy!")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Aleo.com scraper s Playwright')
    parser.add_argument('--categories', nargs='+', help='Seznam URL kategori√≠ k prohled√°n√≠')
    parser.add_argument('--category-file', default='kategorie.txt', help='Soubor s kategoriemi (1 URL na ≈ô√°dek)')
    parser.add_argument('--max', type=int, default=10, help='Maxim√°ln√≠ poƒçet firem z ka≈æd√© kategorie')
    
    args = parser.parse_args()
    
    # Naƒçten√≠ kategori√≠
    categories = []
    
    if args.categories:
        categories = args.categories
    elif os.path.exists(args.category_file):
        with open(args.category_file, 'r', encoding='utf-8') as f:
            categories = [line.strip() for line in f if line.strip() and not line.startswith('#')]
    
    if not categories:
        print("‚ùå Chyba: Nebyly zad√°ny ≈æ√°dn√© kategorie!")
        print("Pou≈æijte: --categories URL1 URL2 ... nebo vytvo≈ôte soubor kategorie.txt")
        exit(1)
    
    # Spu≈°tƒõn√≠ async funkce
    asyncio.run(main(categories, args.max))
