"""
KOMPLETN√ç ALEO.COM SCRAPER
1. Z√≠sk√° n√°zvy firem z aleo.com kategori√≠
2. Najde jejich weby p≈ôes Google search
3. Najde emaily na webech nebo p≈ôes Google
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import re
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import os
import argparse

# === KONFIGURACE ===
OUTPUT_DIR = "output"
MAX_CLOUDFLARE_WAIT = 120
EMAIL_PATTERN = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b')

def create_output_dir():
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)

def setup_driver():
    chrome_options = Options()
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    chrome_options.add_argument('--disable-blink-features=AutomationControlled')
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    
    driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
        'source': '''
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            })
        '''
    })
    
    return driver

def wait_for_cloudflare(driver):
    print("\n‚ö†Ô∏è  CLOUDFLARE DETEKOV√ÅNA!")
    print("="*60)
    print("KLIKNƒöTE NA CHECKBOX 'Verify you are human'")
    print(f"ƒåek√°m max. {MAX_CLOUDFLARE_WAIT} sekund...")
    print("="*60)
    
    start_time = time.time()
    while time.time() - start_time < MAX_CLOUDFLARE_WAIT:
        if "cloudflare" not in driver.page_source.lower() and "challenge" not in driver.page_source.lower():
            print("‚úÖ Cloudflare vy≈ôe≈°ena!")
            return True
        time.sleep(1)
    
    return False

def extract_company_names(driver, category_url, max_companies):
    """Krok 1: Extrakce n√°zv≈Ø firem z aleo.com"""
    print(f"\nüìÇ Kategorie: {category_url}")
    
    driver.get(category_url)
    
    if "cloudflare" in driver.page_source.lower():
        if not wait_for_cloudflare(driver):
            return []
    
    print("  ‚è≥ Naƒç√≠t√°m firmy...")
    time.sleep(5)
    
    # Scrollov√°n√≠ pro naƒçten√≠ v≈°ech firem
    for i in range(5):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)
    
    companies = []
    
    try:
        elements = driver.find_elements(By.CLASS_NAME, "catalog-row-first-line__company-name")
        print(f"  ‚úÖ Nalezeno {len(elements)} firem")
        
        for elem in elements[:max_companies]:
            try:
                name = elem.text.strip()
                if name and len(name) > 2:
                    companies.append(name)
            except:
                continue
    except Exception as e:
        print(f"  ‚ùå Chyba: {str(e)}")
    
    return list(dict.fromkeys(companies))  # Odstranƒõn√≠ duplik√°t≈Ø

def search_google_maps(driver, company_name):
    """Hled√° firmu na Google Maps a z√≠sk√° kontakty"""
    try:
        short_name = company_name.split('SP√ì≈ÅKA')[0].strip()
        short_name = short_name.split(' SP.')[0].strip()
        
        print(f"    üó∫Ô∏è  Google Maps: {short_name}")
        
        # Otev≈ô√≠t Google Maps
        maps_url = f"https://www.google.com/maps/search/{requests.utils.quote(short_name + ' Poland')}"
        driver.get(maps_url)
        time.sleep(6)  # Del≈°√≠ ƒçek√°n√≠ pro Maps
        
        result = {'website': None, 'email': None, 'phone': None}
        
        try:
            # Prostƒõ kliknout na prvn√≠ v√Ωsledek - zjednodu≈°enƒõ
            try:
                # R≈Øzn√© mo≈æn√© selektory pro prvn√≠ v√Ωsledek
                selectors = [
                    "a[href*='maps/place']",
                    "div[role='article'] a",
                    ".Nv2PK a"
                ]
                
                clicked = False
                for selector in selectors:
                    try:
                        elements = driver.find_elements(By.CSS_SELECTOR, selector)
                        if elements:
                            elements[0].click()
                            clicked = True
                            time.sleep(4)
                            break
                    except:
                        continue
                
                if not clicked:
                    print(f"    ‚ö†Ô∏è  Nenalezen v√Ωsledek v Maps")
                    return result
                
            except Exception as e:
                print(f"    ‚ö†Ô∏è  Klik chyba: {str(e)[:50]}")
                return result
            
            # Z√≠skat HTML po kliknut√≠
            page_source = driver.page_source
            
            # Hledat web - jednodu≈°≈°√≠ regex
            urls = re.findall(r'https?://[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}(?:/[^\s"<>]*)?', page_source)
            
            for url in urls:
                # Filtrovat Google dom√©ny
                if not any(skip in url.lower() for skip in 
                    ['google.', 'gstatic.', 'googleapis.', 'facebook.', 'instagram.', 'twitter.', 'youtube.']):
                    result['website'] = url
                    print(f"    üåê Web z Maps: {url}")
                    break
            
            # Hledat email
            emails = EMAIL_PATTERN.findall(page_source)
            for email in emails:
                if not any(skip in email.lower() for skip in 
                    ['google.', 'example.', 'test@', 'noreply', '@gstatic', '.png', '.jpg']):
                    result['email'] = email
                    print(f"    ‚úÖ Email z Maps: {email}")
                    break
            
        except Exception as e:
            print(f"    ‚ö†Ô∏è  Maps parse chyba: {str(e)[:50]}")
        
        return result
        
    except Exception as e:
        print(f"    ‚ö†Ô∏è  Maps chyba: {str(e)[:50]}")
        return {'website': None, 'email': None, 'phone': None}

def google_search_website(driver, company_name):
    """Najde web firmy p≈ôes Google search (Selenium)"""
    try:
        short_name = company_name.split('SP√ì≈ÅKA')[0].strip()
        short_name = short_name.split(' SP.')[0].strip()
        short_name = short_name.split(' S.A.')[0].strip()
        
        query = f"{short_name} Poland"
        
        print(f"    üîç Google: {query}")
        
        # Otev≈ô√≠t Google
        url = f"https://www.google.com/search?q={requests.utils.quote(query)}&hl=pl"
        driver.get(url)
        time.sleep(3)
        
        # Z√≠skat HTML
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        # Hledat linky
        links = soup.find_all('a')
        
        for link in links:
            href = link.get('href', '')
            
            # Google redirect pattern
            if '/url?q=' in href:
                match = re.search(r'/url\?q=([^&]+)', href)
                if match:
                    found_url = requests.utils.unquote(match.group(1))
                    
                    # Filtrovat nesmysly
                    skip_domains = ['google.', 'facebook.', 'linkedin.', 'wikipedia.', 'aleo.com', 'youtube.']
                    if not any(skip in found_url.lower() for skip in skip_domains):
                        if found_url.startswith('http'):
                            print(f"    üåê Web: {found_url}")
                            return found_url
            
            # P≈ô√≠m√Ω link
            elif href.startswith('http'):
                skip_domains = ['google.', 'facebook.', 'linkedin.', 'wikipedia.', 'aleo.com', 'youtube.']
                if not any(skip in href.lower() for skip in skip_domains):
                    print(f"    üåê Web: {href}")
                    return href
        
        return None
        
    except Exception as e:
        print(f"    ‚ö†Ô∏è  Chyba: {str(e)[:50]}")
        return None

def google_search_email(driver, company_name):
    """Najde email firmy p≈ôes Google (Selenium)"""
    try:
        short_name = company_name.split('SP√ì≈ÅKA')[0].strip()
        short_name = short_name.split(' SP.')[0].strip()
        
        query = f"{short_name} email kontakt Poland"
        
        print(f"    üìß Google email: {query}")
        
        # Otev≈ô√≠t Google
        url = f"https://www.google.com/search?q={requests.utils.quote(query)}&hl=pl"
        driver.get(url)
        time.sleep(3)
        
        # Hledat emaily v HTML
        emails = EMAIL_PATTERN.findall(driver.page_source)
        
        for email in emails:
            # Filtrovat nesmysly
            skip = ['google.', 'youtube.', 'example.', 'noreply', 'privacy', '.png', '.jpg', '@gstatic']
            if not any(skip in email.lower() for skip in skip):
                print(f"    ‚úÖ Email: {email}")
                return email
        
        return None
        
    except Exception as e:
        print(f"    ‚ö†Ô∏è  Chyba: {str(e)[:50]}")
        return None

def find_email_on_website(url):
    """Krok 3b: Hled√° email p≈ô√≠mo na webu firmy"""
    if not url:
        return None
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        pages = [
            url,
            f"{url}/kontakt",
            f"{url}/contact",
            f"{url}/kontakty",
            f"{url}/o-nas"
        ]
        
        for page_url in pages:
            try:
                response = requests.get(page_url, headers=headers, timeout=10)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    text = soup.get_text()
                    
                    emails = EMAIL_PATTERN.findall(text)
                    
                    for email in emails:
                        if not any(skip in email.lower() for skip in 
                            ['example.', 'test@', 'noreply', 'wix.com', 'domain.']):
                            return email
            except:
                continue
        
        return None
    except:
        return None

def scrape_category(driver, category_url, max_companies):
    """Zpracuje jednu kategorii - kompletn√≠ workflow"""
    
    # KROK 1: Z√≠skat n√°zvy firem z aleo.com
    company_names = extract_company_names(driver, category_url, max_companies)
    
    if not company_names:
        print("  ‚ùå ≈Ω√°dn√© firmy nenalezeny")
        return []
    
    print(f"\n‚úÖ Nalezeno {len(company_names)} firem, zpracov√°v√°m...")
    
    results = []
    
    # KROK 2 & 3: Pro ka≈ædou firmu naj√≠t web a email
    for idx, company_name in enumerate(company_names, 1):
        print(f"\n  [{idx}/{len(company_names)}] {company_name}")
        
        website = None
        email = None
        phone = None
        
        # PRIORITA 1: Google search pro web
        website = google_search_website(driver, company_name)
        
        # PRIORITA 2: Email z webu nebo z Google
        if website:
            print(f"    üìß Hled√°m email na webu...")
            email = find_email_on_website(website)
            if email:
                print(f"    ‚úÖ Email z webu: {email}")
        
        # PRIORITA 3: Pokud nen√≠ email, zkus Google
        if not email:
            email = google_search_email(driver, company_name)
        
        if email:
            print(f"    ‚úÖ Kompletn√≠!")
        else:
            print(f"    ‚ö†Ô∏è  Email nenalezen")
        
        results.append({
            'name': company_name,
            'website': website or '',
            'email': email or '',
            'phone': phone or '',
            'category': category_url
        })
        
        # Pauza mezi firmami
        time.sleep(2)
    
    return results

def main(categories, max_companies_per_category):
    print("\n" + "="*60)
    print("üöÄ KOMPLETN√ç ALEO.COM SCRAPER")
    print("="*60)
    
    all_results = []
    driver = setup_driver()
    
    try:
        print(f"\n‚úÖ Kategorie: {len(categories)}")
        for cat in categories:
            print(f"  ‚Ä¢ {cat}")
        
        print(f"\n‚öôÔ∏è  Limit: {max_companies_per_category} firem/kategorii")
        print("üöÄ Spou≈°t√≠m Chrome...")
        
        for idx, category_url in enumerate(categories, 1):
            print(f"\n{'='*60}")
            print(f"KATEGORIE {idx}/{len(categories)}")
            print("="*60)
            
            results = scrape_category(driver, category_url, max_companies_per_category)
            all_results.extend(results)
            
            if idx < len(categories):
                print(f"\n‚è∏Ô∏è  Pauza 5s p≈ôed dal≈°√≠ kategori√≠...")
                time.sleep(5)
        
    finally:
        driver.quit()
    
    # Export
    if all_results:
        create_output_dir()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        csv_file = os.path.join(OUTPUT_DIR, f"aleo_complete_{timestamp}.csv")
        xlsx_file = os.path.join(OUTPUT_DIR, f"aleo_complete_{timestamp}.xlsx")
        
        df = pd.DataFrame(all_results)
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        df.to_excel(xlsx_file, index=False, engine='openpyxl')
        
        print("\n" + "="*60)
        print("‚úÖ HOTOVO!")
        print("="*60)
        print(f"Celkem firem: {len(all_results)}")
        print(f"Firem s emailem: {sum(1 for r in all_results if r['email'])}")
        print(f"Firem s webem: {sum(1 for r in all_results if r['website'])}")
        print(f"Firem s telefonem: {sum(1 for r in all_results if r.get('phone'))}")
        print(f"\nüìÅ Soubory:")
        print(f"  ‚Ä¢ {csv_file}")
        print(f"  ‚Ä¢ {xlsx_file}")
    else:
        print("\n‚ùå ≈Ω√°dn√© v√Ωsledky!")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Kompletn√≠ Aleo.com scraper')
    parser.add_argument('--categories', nargs='+', help='URL kategori√≠')
    parser.add_argument('--category-file', default='kategorie.txt', help='Soubor s kategoriemi')
    parser.add_argument('--max', type=int, default=10, help='Max firem z kategorie')
    
    args = parser.parse_args()
    
    categories = []
    if args.categories:
        categories = args.categories
    elif os.path.exists(args.category_file):
        with open(args.category_file, 'r', encoding='utf-8') as f:
            categories = [line.strip() for line in f if line.strip() and not line.startswith('#')]
    
    if not categories:
        print("‚ùå Chyba: ≈Ω√°dn√© kategorie!")
        print("Pou≈æijte: --categories URL nebo vytvo≈ôte kategorie.txt")
        exit(1)
    
    main(categories, args.max)
