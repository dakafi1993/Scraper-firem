#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Aleo.com Semi-Manual Scraper
Å˜eÅ¡enÃ­ pro weby chrÃ¡nÄ›nÃ© Cloudflare - manuÃ¡lnÃ­ prochÃ¡zenÃ­ s automatickÃ½m extrakcÃ­
"""

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import pandas as pd
import re
import time
from datetime import datetime
from pathlib import Path
import json

class SemiManualScraper:
    """Semi-manuÃ¡lnÃ­ scraper pro weby s Cloudflare ochranou"""
    
    def __init__(self):
        self.results = []
        self.driver = None
        self._init_driver()
    
    def _init_driver(self):
        """Inicializace Chrome prohlÃ­Å¾eÄe"""
        print("ğŸš€ Inicializuji prohlÃ­Å¾eÄ...")
        chrome_options = Options()
        # NEPOUÅ½ÃVAT headless mode - musÃ­ bÃ½t viditelnÃ½ pro Cloudflare
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option('excludeSwitches', ['enable-automation'])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
        self.driver.maximize_window()
        print("âœ“ ProhlÃ­Å¾eÄ pÅ™ipraven\n")
    
    def _extract_email(self, text: str) -> str:
        """Extrahuje e-mailovou adresu z textu"""
        if not text:
            return None
        email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        emails = re.findall(email_pattern, text)
        if emails:
            filtered = [e for e in emails if not any(
                placeholder in e.lower() 
                for placeholder in ['example.com', 'domain.com', 'email.cz']
            )]
            return filtered[0] if filtered else None
        return None
    
    def wait_for_user(self, message: str):
        """PoÄkÃ¡ na potvrzenÃ­ uÅ¾ivatele"""
        input(f"\n{'='*60}\n{message}\nStisknÄ›te ENTER pro pokraÄovÃ¡nÃ­...\n{'='*60}\n")
    
    def scrape_current_page(self) -> list:
        """Extrahuje data z aktuÃ¡lnÄ› naÄtenÃ© strÃ¡nky"""
        print("ğŸ“Š Analyzuji aktuÃ¡lnÃ­ strÃ¡nku...")
        time.sleep(2)
        
        soup = BeautifulSoup(self.driver.page_source, 'lxml')
        companies = []
        
        # HledÃ¡nÃ­ vÅ¡ech odkazÅ¯
        all_links = soup.find_all('a', href=True)
        
        print(f"   Nalezeno {len(all_links)} odkazÅ¯ na strÃ¡nce")
        print("   HledÃ¡m odkazy na spoleÄnosti...")
        
        for link in all_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            # FiltrovÃ¡nÃ­ odkazÅ¯, kterÃ© vypadajÃ­ jako profily firem
            if text and len(text) > 3:
                # RÅ¯znÃ© moÅ¾nÃ© vzory URL
                if any(pattern in href.lower() for pattern in 
                       ['firma', 'company', 'detail', 'profil', 'subjekt']):
                    
                    full_url = href if href.startswith('http') else f"https://www.aleo.com{href}"
                    
                    companies.append({
                        'name': text,
                        'url': full_url
                    })
        
        # OdstranÄ›nÃ­ duplikÃ¡tÅ¯
        seen = set()
        unique_companies = []
        for comp in companies:
            if comp['url'] not in seen:
                seen.add(comp['url'])
                unique_companies.append(comp)
        
        print(f"   âœ“ Nalezeno {len(unique_companies)} unikÃ¡tnÃ­ch spoleÄnostÃ­\n")
        return unique_companies
    
    def scrape_company_detail(self, company: dict) -> dict:
        """ZÃ­skÃ¡ detaily o jednÃ© spoleÄnosti"""
        result = {
            'nÃ¡zev_spoleÄnosti': company['name'],
            'email': None,
            'zdroj': None,
            'url_profilu': company['url']
        }
        
        try:
            print(f"   NaÄÃ­tÃ¡m: {company['name'][:50]}...")
            self.driver.get(company['url'])
            time.sleep(3)
            
            soup = BeautifulSoup(self.driver.page_source, 'lxml')
            page_text = soup.get_text()
            
            # HledÃ¡nÃ­ e-mailu
            email = self._extract_email(page_text)
            
            if email:
                result['email'] = email
                result['zdroj'] = 'Profil spoleÄnosti'
                print(f"   âœ“ Email: {email}")
            else:
                print(f"   âœ— Email nenalezen")
                result['zdroj'] = 'Email nenalezen'
                
        except Exception as e:
            print(f"   âœ— Chyba: {e}")
            result['zdroj'] = f'Chyba: {str(e)[:50]}'
        
        return result
    
    def run_manual_mode(self):
        """HlavnÃ­ reÅ¾im - manuÃ¡lnÃ­ prochÃ¡zenÃ­ s automatickou extrakcÃ­"""
        print("\n" + "="*60)
        print("SEMI-MANUÃLNÃ REÅ½IM SCRAPERU")
        print("="*60)
        print("\nTento reÅ¾im funguje takto:")
        print("1. RuÄnÄ› otevÅ™ete strÃ¡nku katalogu v prohlÃ­Å¾eÄi")
        print("2. VyÅ™eÅ¡te Cloudflare challenge (pokud se zobrazÃ­)")
        print("3. Skript automaticky extrahuje data z aktuÃ¡lnÃ­ strÃ¡nky")
        print("4. ProchÃ¡zejte jednotlivÃ© profily spoleÄnostÃ­\n")
        
        self.wait_for_user(
            "âš ï¸  NYNÃ:\n"
            "1. OtevÅ™e se Chrome prohlÃ­Å¾eÄ\n"
            "2. RuÄnÄ› pÅ™ejdÄ›te na https://www.aleo.com/firmy\n"
            "3. VyÅ™eÅ¡te Cloudflare kontrolu (kliknÄ›te na checkbox)\n"
            "4. PoÄkejte, aÅ¾ se strÃ¡nka plnÄ› naÄte"
        )
        
        # OtevÅ™enÃ­ strÃ¡nky (uÅ¾ivatel vyÅ™eÅ¡Ã­ Cloudflare)
        self.driver.get("https://www.aleo.com/firmy")
        
        self.wait_for_user(
            "âœ… UjistÄ›te se, Å¾e:\n"
            "   - Cloudflare challenge je vyÅ™eÅ¡en\n"
            "   - StrÃ¡nka s firmami je plnÄ› naÄtena\n"
            "   - VidÃ­te seznam firem"
        )
        
        # Extrakce spoleÄnostÃ­ z aktuÃ¡lnÃ­ strÃ¡nky
        companies = self.scrape_current_page()
        
        if not companies:
            print("âŒ Nebyly nalezeny Å¾Ã¡dnÃ© spoleÄnosti!")
            print("   MoÅ¾nÃ¡ je potÅ™eba upravit CSS selektory v kÃ³du.")
            self.wait_for_user("Zkontrolujte strÃ¡nku a stisknÄ›te ENTER")
            return
        
        print(f"\nğŸ“‹ Celkem nalezeno: {len(companies)} spoleÄnostÃ­")
        print("\nPÅ™Ã­klady:")
        for i, comp in enumerate(companies[:5], 1):
            print(f"   {i}. {comp['name'][:50]}")
        
        # Dotaz na zpracovÃ¡nÃ­
        process = input(f"\nâ“ Zpracovat tÄ›chto {len(companies)} spoleÄnostÃ­? (y/n/ÄÃ­slo): ").strip().lower()
        
        if process == 'n':
            print("UkonÄuji...")
            return
        elif process.isdigit():
            companies = companies[:int(process)]
            print(f"âœ“ ZpracovÃ¡vÃ¡m prvnÃ­ch {process} spoleÄnostÃ­")
        
        # ZpracovÃ¡nÃ­ jednotlivÃ½ch spoleÄnostÃ­
        print(f"\n{'='*60}")
        print("ZPRACOVÃVÃM DETAILY SPOLEÄŒNOSTÃ")
        print("="*60 + "\n")
        
        for i, company in enumerate(companies, 1):
            print(f"[{i}/{len(companies)}]")
            result = self.scrape_company_detail(company)
            self.results.append(result)
            time.sleep(2)  # SluÅ¡nÃ¡ prodleva
        
        # UloÅ¾enÃ­ vÃ½sledkÅ¯
        self.save_results()
        
        print("\n" + "="*60)
        print("âœ… SCRAPING DOKONÄŒEN!")
        print(f"   ZpracovÃ¡no: {len(self.results)} spoleÄnostÃ­")
        print(f"   S e-mailem: {sum(1 for r in self.results if r['email'])}")
        print("="*60 + "\n")
    
    def run_current_page_only(self):
        """ReÅ¾im pro extrakci pouze z aktuÃ¡lnÃ­ strÃ¡nky"""
        print("\n" + "="*60)
        print("REÅ½IM: ANALÃZA AKTUÃLNÃ STRÃNKY")
        print("="*60 + "\n")
        
        self.wait_for_user(
            "OtevÅ™e se prohlÃ­Å¾eÄ.\n"
            "RuÄnÄ› pÅ™ejdÄ›te na strÃ¡nku, kterou chcete analyzovat."
        )
        
        self.driver.get("https://www.aleo.com")
        
        self.wait_for_user("PÅ™ejdÄ›te na strÃ¡nku k analÃ½ze a stisknÄ›te ENTER")
        
        companies = self.scrape_current_page()
        
        if companies:
            # Export do CSV
            df = pd.DataFrame(companies)
            output_dir = Path('output')
            output_dir.mkdir(exist_ok=True)
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            csv_path = output_dir / f'found_companies_{timestamp}.csv'
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            print(f"\nâœ“ Seznam spoleÄnostÃ­ uloÅ¾en: {csv_path}")
        
        self.close()
    
    def save_results(self):
        """UloÅ¾enÃ­ vÃ½sledkÅ¯"""
        if not self.results:
            return
        
        df = pd.DataFrame(self.results)
        output_dir = Path('output')
        output_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # CSV
        csv_path = output_dir / f'aleo_manual_{timestamp}.csv'
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"   âœ“ CSV: {csv_path}")
        
        # Excel
        excel_path = output_dir / f'aleo_manual_{timestamp}.xlsx'
        df.to_excel(excel_path, index=False, engine='openpyxl')
        print(f"   âœ“ Excel: {excel_path}")
        
        # Statistiky
        stats = {
            'celkem': len(self.results),
            'nalezeno_emailÅ¯': df['email'].notna().sum(),
            'nenalezeno': df['email'].isna().sum(),
            'ÃºspÄ›Å¡nost': f"{(df['email'].notna().sum() / len(self.results) * 100):.1f}%"
        }
        
        stats_path = output_dir / f'stats_manual_{timestamp}.json'
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
    
    def close(self):
        """UzavÅ™e prohlÃ­Å¾eÄ"""
        if self.driver:
            self.driver.quit()
            print("\nâœ“ ProhlÃ­Å¾eÄ uzavÅ™en")


def main():
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘        ALEO.COM SEMI-MANUÃLNÃ SCRAPER                     â•‘
â•‘     Å˜eÅ¡enÃ­ pro weby chrÃ¡nÄ›nÃ© Cloudflare                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    print("\nVyberte reÅ¾im:")
    print("1. PlnÃ½ scraping (s prochÃ¡zenÃ­m profilÅ¯)")
    print("2. Pouze analÃ½za aktuÃ¡lnÃ­ strÃ¡nky")
    
    choice = input("\nVolba (1/2): ").strip()
    
    scraper = SemiManualScraper()
    
    try:
        if choice == "1":
            scraper.run_manual_mode()
        elif choice == "2":
            scraper.run_current_page_only()
        else:
            print("NeplatnÃ¡ volba!")
    except KeyboardInterrupt:
        print("\n\nâš ï¸  PÅ™eruÅ¡eno uÅ¾ivatelem")
    finally:
        scraper.close()


if __name__ == "__main__":
    main()
