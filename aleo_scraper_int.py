#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Aleo.com International Scraper s Google vyhledÃ¡vÃ¡nÃ­m
AutomatickÃ© stahovÃ¡nÃ­ dat firem a vyhledÃ¡vÃ¡nÃ­ emailÅ¯
"""

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.keys import Keys
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import pandas as pd
import re
import time
from datetime import datetime
from pathlib import Path
import json
import logging

# Konfigurace loggingu
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraper_int.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class AleoInternationalScraper:
    """Scraper pro aleo.com/int s vyhledÃ¡vÃ¡nÃ­m emailÅ¯"""
    
    def __init__(self):
        self.results = []
        self.driver = None
        self.companies = []
        self._init_driver()
    
    def _init_driver(self):
        """Inicializace Chrome prohlÃ­Å¾eÄe"""
        logger.info("ğŸš€ Inicializuji prohlÃ­Å¾eÄ...")
        chrome_options = Options()
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_experimental_option('excludeSwitches', ['enable-automation'])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # Headless mode pro rychlejÅ¡Ã­ bÄ›h
        # chrome_options.add_argument('--headless')  # Odkomentovat pro bÄ›h na pozadÃ­
        
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
        self.driver.maximize_window()
        logger.info("âœ“ ProhlÃ­Å¾eÄ pÅ™ipraven\n")
    
    def _extract_email(self, text: str) -> str:
        """Extrahuje e-mailovou adresu z textu"""
        if not text:
            return None
        
        # Regex pattern pro e-mailovÃ© adresy
        email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        emails = re.findall(email_pattern, text)
        
        if emails:
            # FiltrovÃ¡nÃ­ bÄ›Å¾nÃ½ch placeholder e-mailÅ¯
            filtered = [e for e in emails if not any(
                placeholder in e.lower() 
                for placeholder in ['example.com', 'domain.com', 'email.cz', 
                                   'test.com', 'sample.com', 'xxx.com']
            )]
            return filtered[0] if filtered else None
        return None
    
    def wait_for_cloudflare(self):
        """PoÄkÃ¡ na vyÅ™eÅ¡enÃ­ Cloudflare challenge"""
        logger.info("â³ ÄŒekÃ¡m na Cloudflare challenge...")
        logger.info("   âš ï¸  NYNÃ: Pokud vidÃ­te CAPTCHA, vyÅ™eÅ¡te ji RUÄŒNÄš v prohlÃ­Å¾eÄi!")
        logger.info("   Po vyÅ™eÅ¡enÃ­ strÃ¡nka automaticky pokraÄuje...")
        
        max_wait = 120  # 2 minuty
        start_time = time.time()
        
        while time.time() - start_time < max_wait:
            try:
                page_source = self.driver.page_source.lower()
                current_url = self.driver.current_url.lower()
                
                # Kontrola, zda je strÃ¡nka naÄtena
                if ("ovÄ›Å™ujeme" not in page_source and 
                    "challenge" not in page_source and
                    "cloudflare" not in page_source and
                    "__cf_chl" not in current_url):
                    logger.info("âœ“ Cloudflare vyÅ™eÅ¡en!")
                    time.sleep(3)  # Extra Äas na plnÃ© naÄtenÃ­
                    return True
            except:
                pass
            time.sleep(2)
        
        logger.warning("âš ï¸  Timeout pÅ™i ÄekÃ¡nÃ­ na Cloudflare")
        logger.info("   Zkuste: RuÄnÄ› proklikat CAPTCHA v prohlÃ­Å¾eÄi")
        input("\n   >>> StisknÄ›te ENTER po vyÅ™eÅ¡enÃ­ CAPTCHA <<<\n")
        time.sleep(3)
        return True
    
    def scrape_company_list(self, url: str, max_companies: int = None):
        """ZÃ­skÃ¡ seznam spoleÄnostÃ­ z aleo.com/int"""
        logger.info(f"ğŸ“‹ NaÄÃ­tÃ¡m seznam spoleÄnostÃ­ z: {url}")
        
        try:
            self.driver.get(url)
            self.wait_for_cloudflare()
            
            # Scroll pro naÄtenÃ­ lazy-loaded obsahu
            logger.info("ğŸ“œ Scrolluji strÃ¡nku pro naÄtenÃ­ vÅ¡ech dat...")
            last_height = self.driver.execute_script("return document.body.scrollHeight")
            
            for i in range(5):  # Max 5 scrollÅ¯
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)
                new_height = self.driver.execute_script("return document.body.scrollHeight")
                if new_height == last_height:
                    break
                last_height = new_height
            
            # Scroll zpÄ›t nahoru
            self.driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(2)
            
            # Extrakce dat
            soup = BeautifulSoup(self.driver.page_source, 'lxml')
            
            # UloÅ¾it HTML pro analÃ½zu
            with open('aleo_int_page.html', 'w', encoding='utf-8') as f:
                f.write(self.driver.page_source)
            logger.info("ğŸ’¾ HTML strÃ¡nky uloÅ¾eno do aleo_int_page.html pro analÃ½zu")
            
            # HledÃ¡nÃ­ rÅ¯znÃ½ch vzorÅ¯ odkazÅ¯ na firmy
            all_links = soup.find_all('a', href=True)
            
            logger.info(f"ğŸ” Analyzuji {len(all_links)} odkazÅ¯...")
            
            seen_urls = set()
            
            # METODA 1: HledÃ¡nÃ­ specifickÃ½ch CSS tÅ™Ã­d pro firmy
            # Zkuste najÃ­t karty/poloÅ¾ky firem
            company_containers = soup.find_all(['div', 'article', 'li'], class_=re.compile(r'company|firma|card|item|listing|result', re.I))
            
            logger.info(f"ğŸ“¦ Nalezeno {len(company_containers)} potenciÃ¡lnÃ­ch kontejnerÅ¯")
            
            for container in company_containers:
                # Hledat odkazy uvnitÅ™ kontejneru
                link = container.find('a', href=True)
                if link:
                    href = link.get('href', '')
                    text = link.get_text(strip=True) or container.get_text(strip=True)
                    
                    # MusÃ­ obsahovat /int/ a bÃ½t dostateÄnÄ› dlouhÃ¡ cesta
                    if '/int/' in href and href.count('/') >= 4:
                        full_url = href if href.startswith('http') else f"https://aleo.com{href}"
                        
                        # Filtrovat systÃ©movÃ© strÃ¡nky
                        if not any(skip in href.lower() for skip in 
                                 ['cookie', 'privacy', 'about', 'terms', 'login', 'register']):
                            if full_url not in seen_urls and len(text) > 3:
                                self.companies.append({
                                    'name': text[:100],  # Omezit dÃ©lku
                                    'url': full_url
                                })
                                seen_urls.add(full_url)
            
            # METODA 2: Pokud prvnÃ­ metoda nenaÅ¡la nic, pouÅ¾Ã­t obecnÄ›jÅ¡Ã­ pÅ™Ã­stup
            if len(self.companies) == 0:
                logger.warning("âš ï¸  Metoda 1 nenaÅ¡la firmy, zkouÅ¡Ã­m obecnÄ›jÅ¡Ã­ pÅ™Ã­stup...")
                
                for link in all_links:
                    href = link.get('href', '')
                    text = link.get_text(strip=True)
                    
                    # Hledat odkazy, kterÃ© vypadajÃ­ jako profily firem
                    # MusÃ­ obsahovat /int/ a mÃ­t vÃ­ce neÅ¾ 3 ÃºrovnÄ›
                    if '/int/' in href and href.count('/') >= 4:
                        # VylouÄit navigaci a systÃ©movÃ© strÃ¡nky
                        if not any(skip in href.lower() for skip in 
                                 ['about', 'cookie', 'privacy', 'terms', 'faq', 
                                  'help', 'login', 'register', 'contact']):
                            
                            full_url = href if href.startswith('http') else f"https://aleo.com{href}"
                            
                            if full_url not in seen_urls and text and len(text) > 3 and len(text) < 150:
                                self.companies.append({
                                    'name': text,
                                    'url': full_url
                                })
                                seen_urls.add(full_url)
            
            # METODA 3: Pokud stÃ¡le nic, analyzovat vÅ¡echny odkazy
            if len(self.companies) == 0:
                logger.warning("âš ï¸  StandardnÃ­ metody nenaÅ¡ly firmy")
                logger.info("ğŸ“Š Zobrazuji vÅ¡echny nalezenÃ© odkazy pro manuÃ¡lnÃ­ kontrolu:")
                
                unique_links = []
                for link in all_links[:30]:  # PrvnÃ­ 30 odkazÅ¯
                    href = link.get('href', '')
                    text = link.get_text(strip=True)
                    if href and text:
                        logger.info(f"   â€¢ {text[:60]} â†’ {href[:60]}")
                        unique_links.append({'text': text, 'href': href})
            
            logger.info(f"âœ“ Nalezeno {len(self.companies)} potenciÃ¡lnÃ­ch spoleÄnostÃ­")
            
            if max_companies:
                self.companies = self.companies[:max_companies]
                logger.info(f"ğŸ“Š Omezeno na {max_companies} spoleÄnostÃ­")
            
            # ZobrazenÃ­ vzorku
            if self.companies:
                logger.info("\nğŸ“‹ PÅ™Ã­klady nalezenÃ½ch spoleÄnostÃ­:")
                for i, comp in enumerate(self.companies[:5], 1):
                    logger.info(f"   {i}. {comp['name'][:60]}")
            
        except Exception as e:
            logger.error(f"âŒ Chyba pÅ™i naÄÃ­tÃ¡nÃ­ seznamu: {e}")
    
    def extract_website_from_profile(self, soup: BeautifulSoup) -> str:
        """Extrahuje webovou strÃ¡nku firmy z profilu"""
        # HledÃ¡nÃ­ rÅ¯znÃ½ch vzorÅ¯ odkazÅ¯ na web
        patterns = [
            {'text': re.compile(r'web|website|www|homepage|strÃ¡nk', re.I)},
            {'class': re.compile(r'web|url|link|external', re.I)},
            {'itemprop': 'url'}
        ]
        
        for pattern in patterns:
            links = soup.find_all('a', pattern, href=True)
            for link in links:
                href = link.get('href', '')
                # MusÃ­ bÃ½t externÃ­ odkaz
                if href.startswith('http') and 'aleo.com' not in href:
                    return href
        
        # HledÃ¡nÃ­ v textu (www.firma.cz apod.)
        text = soup.get_text()
        url_pattern = r'https?://(?:www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b'
        urls = re.findall(url_pattern, text)
        for url in urls:
            if 'aleo.com' not in url and any(tld in url for tld in ['.com', '.cz', '.sk', '.eu', '.net', '.org']):
                return url
        
        return None
    
    def search_email_on_website(self, company_name: str, website_url: str = None) -> tuple:
        """VyhledÃ¡ email na webu spoleÄnosti"""
        logger.info(f"   ğŸ” HledÃ¡m web spoleÄnosti: {company_name[:40]}...")
        
        try:
            # Pokud nenÃ­ URL, zkusit odhadnout
            if not website_url:
                # Pokus o vytvoÅ™enÃ­ URL z nÃ¡zvu
                clean_name = re.sub(r'[^a-zA-Z0-9]', '', company_name.lower())
                possible_urls = [
                    f"https://www.{clean_name}.com",
                    f"https://www.{clean_name}.cz",
                    f"https://{clean_name}.com"
                ]
                
                for test_url in possible_urls[:1]:  # Zkusit jen prvnÃ­
                    try:
                        logger.info(f"   ğŸŒ ZkouÅ¡Ã­m: {test_url}")
                        self.driver.get(test_url)
                        time.sleep(3)
                        
                        # Kontrola, zda strÃ¡nka existuje (ne 404)
                        if "404" not in self.driver.title.lower() and len(self.driver.page_source) > 1000:
                            website_url = test_url
                            break
                    except:
                        continue
            
            if website_url:
                logger.info(f"   ğŸ“„ NaÄÃ­tÃ¡m web: {website_url[:50]}...")
                
                # OtevÅ™enÃ­ webu
                self.driver.get(website_url)
                time.sleep(3)
                
                soup = BeautifulSoup(self.driver.page_source, 'lxml')
                
                # HledÃ¡nÃ­ kontaktnÃ­ strÃ¡nky
                contact_links = soup.find_all('a', href=True, text=re.compile(r'contact|kontakt', re.I))
                
                if contact_links:
                    contact_url = contact_links[0].get('href', '')
                    if not contact_url.startswith('http'):
                        from urllib.parse import urljoin
                        contact_url = urljoin(website_url, contact_url)
                    
                    logger.info(f"   ğŸ“ NaÅ¡el jsem kontaktnÃ­ strÃ¡nku: {contact_url[:50]}...")
                    self.driver.get(contact_url)
                    time.sleep(2)
                    soup = BeautifulSoup(self.driver.page_source, 'lxml')
                
                # HledÃ¡nÃ­ emailu
                page_text = soup.get_text()
                email = self._extract_email(page_text)
                
                if email:
                    logger.info(f"   âœ“ Email nalezen na webu: {email}")
                    return email, f"Web spoleÄnosti ({website_url})"
                else:
                    logger.info(f"   âœ— Email na webu nenalezen")
                    return None, "Email nenalezen na webu"
            
            logger.info(f"   âš ï¸  Web spoleÄnosti nebyl nalezen")
            return None, "Web nenalezen"
            
        except Exception as e:
            logger.warning(f"   âš ï¸  Chyba pÅ™i hledÃ¡nÃ­ na webu: {e}")
            return None, f"Chyba: {str(e)[:30]}"
    
    def process_company(self, company: dict, search_website: bool = True) -> dict:
        """Zpracuje jednu spoleÄnost"""
        result = {
            'nÃ¡zev_spoleÄnosti': company['name'],
            'email': None,
            'zdroj': None,
            'url_profilu': company['url'],
            'web_spoleÄnosti': None
        }
        
        try:
            # 1. Pokus - profil na aleo.com
            logger.info(f"\n[Firma] {company['name'][:60]}")
            logger.info(f"   ğŸ“„ OtevÃ­rÃ¡m profil: {company['url'][:60]}...")
            
            self.driver.get(company['url'])
            time.sleep(3)
            
            soup = BeautifulSoup(self.driver.page_source, 'lxml')
            page_text = soup.get_text()
            
            # HledÃ¡nÃ­ emailu na profilu
            email = self._extract_email(page_text)
            
            if email:
                result['email'] = email
                result['zdroj'] = 'Profil na aleo.com'
                logger.info(f"   âœ“ Email z profilu: {email}")
                return result
            
            logger.info(f"   âš ï¸  Email na profilu nenalezen")
            
            # 2. Pokus - extrakce webu spoleÄnosti z profilu
            website_url = self.extract_website_from_profile(soup)
            
            if website_url:
                result['web_spoleÄnosti'] = website_url
                logger.info(f"   ğŸŒ Nalezen web v profilu: {website_url[:50]}")
            
            # 3. Pokus - vyhledÃ¡nÃ­ na webu spoleÄnosti
            if search_website:
                email, source = self.search_email_on_website(company['name'], website_url)
                if email:
                    result['email'] = email
                    result['zdroj'] = source
                    return result
            
            result['zdroj'] = 'Email nenalezen'
            
        except Exception as e:
            logger.error(f"   âŒ Chyba: {e}")
            result['zdroj'] = f'Chyba: {str(e)[:50]}'
        
        return result
    
    def run(self, url: str = "https://aleo.com/int/", 
            max_companies: int = None, 
            search_website: bool = True):
        """HlavnÃ­ metoda scraperu"""
        
        logger.info("=" * 70)
        logger.info("ALEO.COM INTERNATIONAL SCRAPER - START")
        logger.info("=" * 70)
        
        # 1. ZÃ­skÃ¡nÃ­ seznamu spoleÄnostÃ­
        self.scrape_company_list(url, max_companies)
        
        if not self.companies:
            logger.error("âŒ Å½Ã¡dnÃ© spoleÄnosti nebyly nalezeny!")
            logger.info("\nğŸ’¡ TIP: Zkuste:")
            logger.info("   1. Zkontrolovat URL")
            logger.info("   2. RuÄnÄ› otevÅ™Ã­t strÃ¡nku a podÃ­vat se na HTML strukturu")
            logger.info("   3. PouÅ¾Ã­t semi-manuÃ¡lnÃ­ reÅ¾im")
            return
        
        # 2. ZpracovÃ¡nÃ­ spoleÄnostÃ­
        logger.info("\n" + "=" * 70)
        logger.info(f"ZPRACOVÃVÃM {len(self.companies)} SPOLEÄŒNOSTÃ")
        logger.info("=" * 70)
        
        for i, company in enumerate(self.companies, 1):
            logger.info(f"\n[{i}/{len(self.companies)}]")
            result = self.process_company(company, search_website)
            self.results.append(result)
            time.sleep(2)  # Prodleva mezi poÅ¾adavky
        
        # 3. UloÅ¾enÃ­ vÃ½sledkÅ¯
        self.save_results()
        
        # 4. Statistiky
        logger.info("\n" + "=" * 70)
        logger.info("âœ… SCRAPING DOKONÄŒEN")
        logger.info("=" * 70)
        logger.info(f"ğŸ“Š Celkem zpracovÃ¡no: {len(self.results)}")
        logger.info(f"âœ“ Nalezeno emailÅ¯: {sum(1 for r in self.results if r['email'])}")
        logger.info(f"âœ— Nenalezeno: {sum(1 for r in self.results if not r['email'])}")
        logger.info(f"ğŸ“ˆ ÃšspÄ›Å¡nost: {sum(1 for r in self.results if r['email']) / len(self.results) * 100:.1f}%")
        logger.info("=" * 70)
    
    def save_results(self):
        """UloÅ¾enÃ­ vÃ½sledkÅ¯"""
        if not self.results:
            logger.warning("âš ï¸  Å½Ã¡dnÃ© vÃ½sledky k uloÅ¾enÃ­")
            return
        
        df = pd.DataFrame(self.results)
        output_dir = Path('output')
        output_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # CSV
        csv_path = output_dir / f'aleo_int_{timestamp}.csv'
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        logger.info(f"\nğŸ’¾ CSV uloÅ¾eno: {csv_path}")
        
        # Excel
        excel_path = output_dir / f'aleo_int_{timestamp}.xlsx'
        df.to_excel(excel_path, index=False, engine='openpyxl')
        logger.info(f"ğŸ’¾ Excel uloÅ¾eno: {excel_path}")
        
        # Statistiky
        stats = {
            'celkem_zpracovÃ¡no': len(self.results),
            'nalezeno_emailÅ¯': int(df['email'].notna().sum()),
            'nenalezeno_emailÅ¯': int(df['email'].isna().sum()),
            'ÃºspÄ›Å¡nost': f"{(df['email'].notna().sum() / len(self.results) * 100):.1f}%",
            'zdroje': {k: int(v) for k, v in df['zdroj'].value_counts().to_dict().items()}
        }
        
        stats_path = output_dir / f'stats_int_{timestamp}.json'
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ Statistiky: {stats_path}")
    
    def close(self):
        """UzavÅ™e prohlÃ­Å¾eÄ"""
        if self.driver:
            self.driver.quit()
            logger.info("\nâœ“ ProhlÃ­Å¾eÄ uzavÅ™en")


def main():
    import argparse
    
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     ALEO.COM INTERNATIONAL SCRAPER                                â•‘
â•‘     AutomatickÃ© stahovÃ¡nÃ­ dat + vyhledÃ¡vÃ¡nÃ­ emailÅ¯               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    parser = argparse.ArgumentParser(description='Aleo.com International Scraper')
    parser.add_argument('--url', type=str, default='https://aleo.com/int/',
                       help='URL pro scraping (vÃ½chozÃ­: https://aleo.com/int/)')
    parser.add_argument('--max', type=int, default=None,
                       help='MaximÃ¡lnÃ­ poÄet spoleÄnostÃ­')
    parser.add_argument('--no-website', action='store_true',
                       help='Nehledat na webech spoleÄnostÃ­ (rychlejÅ¡Ã­)')
    
    args = parser.parse_args()
    
    scraper = AleoInternationalScraper()
    
    try:
        scraper.run(
            url=args.url,
            max_companies=args.max,
            search_website=not args.no_website
        )
    except KeyboardInterrupt:
        logger.info("\n\nâš ï¸  Scraping pÅ™eruÅ¡en uÅ¾ivatelem")
    except Exception as e:
        logger.error(f"\nâŒ KritickÃ¡ chyba: {e}", exc_info=True)
    finally:
        scraper.close()


if __name__ == "__main__":
    main()
