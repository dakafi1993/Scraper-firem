"""
Aleo.com scraper s Google vyhled√°v√°n√≠m firem
1. Z√≠sk√° n√°zvy firem z aleo.com
2. Najde jejich weby p≈ôes Google
3. Extrahuje emaily z web≈Ø
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import re
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import os
import argparse

# === KONFIGURACE ===
OUTPUT_DIR = "output"
MAX_CLOUDFLARE_WAIT = 120
EMAIL_PATTERN = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b')

def create_output_dir():
    """Vytvo≈ô√≠ v√Ωstupn√≠ adres√°≈ô"""
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)

def setup_driver():
    """Nastaven√≠ Chrome driveru"""
    chrome_options = Options()
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    chrome_options.add_argument('--disable-blink-features=AutomationControlled')
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    
    driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
        'source': '''
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            })
        '''
    })
    
    return driver

def wait_for_cloudflare(driver):
    """ƒåek√° na vy≈ôe≈°en√≠ Cloudflare"""
    print("\n‚ö†Ô∏è  CLOUDFLARE DETEKOV√ÅNA!")
    print("="*60)
    print("MANU√ÅLN√ç KROK:")
    print("1. Kliknƒõte na checkbox 'Verify you are human'")
    print("2. Vy≈ôe≈°te p≈ô√≠padnou captcha")
    print(f"\nƒåek√°m max. {MAX_CLOUDFLARE_WAIT} sekund...")
    print("="*60)
    
    start_time = time.time()
    while time.time() - start_time < MAX_CLOUDFLARE_WAIT:
        if "cloudflare" not in driver.page_source.lower() and "challenge" not in driver.page_source.lower():
            print("‚úÖ Cloudflare vy≈ôe≈°ena!")
            return True
        time.sleep(1)
    
    print("‚ùå Timeout - Cloudflare nebyla vy≈ôe≈°ena")
    return False

def extract_company_names_from_page(driver):
    """
    Extrahuje n√°zvy firem A jejich profile URLs ze str√°nky
    """
    print("  üîç Hled√°m n√°zvy firem na str√°nce...")
    
    # ƒåek√°n√≠ na naƒçten√≠
    time.sleep(5)
    
    # Scrollov√°n√≠
    for i in range(3):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)
    
    companies = []
    
    try:
        # Hled√°n√≠ odkaz≈Ø na profily firem
        elements = driver.find_elements(By.CSS_SELECTOR, "a[href*='/pl/firma/']")
        
        if elements:
            print(f"  ‚úÖ Nalezeno {len(elements)} odkaz≈Ø na profily firem")
            
            seen_urls = set()
            for elem in elements:
                try:
                    href = elem.get_attribute('href')
                    text = elem.text.strip()
                    
                    # P≈ôeskoƒçit duplik√°ty a ne≈æ√°douc√≠ odkazy
                    if href and href not in seen_urls:
                        if '/pl/firma/' in href and '/pl/firmy/' not in href:
                            if text and len(text) > 2:
                                seen_urls.add(href)
                                companies.append({
                                    'name': text,
                                    'profile_url': href
                                })
                except:
                    continue
    
    except Exception as e:
        print(f"  ‚ö†Ô∏è  Chyba p≈ôi extrakci: {str(e)}")
    
    return companies

def get_website_from_profile(driver, profile_url):
    """
    Z√≠sk√° web firmy z jej√≠ho profilu na aleo.com
    """
    try:
        print(f"    üìÑ Otev√≠r√°m profil...")
        driver.get(profile_url)
        time.sleep(3)
        
        # Hled√°n√≠ webu v profilu - r≈Øzn√© mo≈æn√© selektory
        web_selectors = [
            (By.CSS_SELECTOR, "a[href^='http']:not([href*='aleo.com']):not([href*='facebook']):not([href*='linkedin'])"),
            (By.XPATH, "//a[starts-with(@href, 'http') and not(contains(@href, 'aleo.com'))]"),
        ]
        
        for by, selector in web_selectors:
            try:
                elements = driver.find_elements(by, selector)
                for elem in elements:
                    href = elem.get_attribute('href')
                    if href and href.startswith('http'):
                        # P≈ôeskoƒçit soci√°ln√≠ s√≠tƒõ
                        if not any(skip in href.lower() for skip in ['facebook', 'linkedin', 'youtube', 'twitter', 'instagram']):
                            print(f"    üåê Web z profilu: {href}")
                            return href
            except:
                continue
        
        # Pokud nenajdeme v elementech, zkus√≠me regex v HTML
        html = driver.page_source
        urls = re.findall(r'href=["\']?(https?://[^"\'>\s]+)', html)
        
        for url in urls:
            if 'aleo.com' not in url and not any(skip in url.lower() for skip in 
                ['facebook', 'linkedin', 'youtube', 'twitter', 'instagram', 'google']):
                print(f"    üåê Web z HTML: {url}")
                return url
        
        return None
        
    except Exception as e:
        print(f"    ‚ö†Ô∏è  Chyba p≈ôi z√≠sk√°v√°n√≠ webu: {str(e)}")
        return None

def find_email_on_website(url):
    """
    Najde email na webu firmy
    """
    if not url:
        return None
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        # Zkus√≠me hlavn√≠ str√°nku a kontaktn√≠ str√°nky
        pages_to_check = [
            url,
            f"{url}/contact",
            f"{url}/kontakt",
            f"{url}/kontakty",
            f"{url}/about",
            f"{url}/o-nas"
        ]
        
        for page_url in pages_to_check:
            try:
                response = requests.get(page_url, headers=headers, timeout=10)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    text = soup.get_text()
                    
                    # Hled√°n√≠ email≈Ø
                    emails = EMAIL_PATTERN.findall(text)
                    
                    # Filtrov√°n√≠ ne≈æ√°douc√≠ch email≈Ø
                    valid_emails = [
                        email for email in emails
                        if not any(skip in email.lower() for skip in 
                                  ['example.', 'test@', 'admin@', 'noreply', 'wix.com', 'domain.'])
                    ]
                    
                    if valid_emails:
                        return valid_emails[0]
            except:
                continue
        
        return None
        
    except Exception as e:
        return None

def scrape_category(driver, category_url, max_companies=10):
    """
    Scrapuje jednu kategorii
    """
    companies = []
    
    print(f"\nüìÇ Kategorie: {category_url}")
    
    try:
        driver.get(category_url)
        
        # Cloudflare check
        if "cloudflare" in driver.page_source.lower() or "challenge" in driver.page_source.lower():
            if not wait_for_cloudflare(driver):
                return companies
        
        # Extrakce n√°zv≈Ø firem
        company_names = extract_company_names_from_page(driver)
        
        if not company_names:
            print("  ‚ùå Nebyly nalezeny ≈æ√°dn√© firmy na str√°nce")
            return companies
        
        print(f"  ‚úÖ Nalezeno {len(company_names)} n√°zv≈Ø firem")
        
        # Omezen√≠ poƒçtu
        company_names = company_names[:max_companies]
        
        # Zpracov√°n√≠ ka≈æd√© firmy
        for idx, company_name in enumerate(company_names, 1):
            print(f"\n  [{idx}/{len(company_names)}] {company_name}")
            
            # Google search pro web
            website = google_search_company(company_name, driver)
            
            # Hled√°n√≠ emailu
            email = None
            if website:
                print(f"    üìß Hled√°m email...")
                email = find_email_on_website(website)
                if email:
                    print(f"    ‚úÖ Email: {email}")
                else:
                    print(f"    ‚ö†Ô∏è  Email nenalezen")
            else:
                print(f"    ‚ö†Ô∏è  Web nenalezen")
            
            companies.append({
                'name': company_name,
                'website': website or '',
                'email': email or '',
                'category': category_url
            })
            
            # Pauza mezi firmami
            time.sleep(2)
        
        print(f"\n‚úÖ Z kategorie zpracov√°no: {len(companies)} firem")
        
    except Exception as e:
        print(f"‚ùå Chyba: {str(e)}")
    
    return companies

def main(categories, max_companies_per_category):
    """Hlavn√≠ funkce"""
    print("\n" + "="*60)
    print("üöÄ ALEO.COM SCRAPER S GOOGLE VYHLED√ÅV√ÅN√çM")
    print("="*60)
    
    all_companies = []
    driver = setup_driver()
    
    try:
        print(f"\n‚úÖ Budu prohled√°vat {len(categories)} kategori√≠:")
        for cat in categories:
            print(f"  ‚Ä¢ {cat}")
        
        print(f"\n‚öôÔ∏è  Limit: {max_companies_per_category} firem z ka≈æd√© kategorie")
        print("üöÄ Spou≈°t√≠m Chrome...")
        
        for idx, category_url in enumerate(categories, 1):
            print(f"\n{'='*60}")
            print(f"KATEGORIE {idx}/{len(categories)}")
            print("="*60)
            
            companies = scrape_category(driver, category_url, max_companies_per_category)
            all_companies.extend(companies)
            
            if idx < len(categories):
                print(f"\n‚è∏Ô∏è  Pauza 5s p≈ôed dal≈°√≠ kategori√≠...")
                time.sleep(5)
        
    finally:
        driver.quit()
    
    # Export
    if all_companies:
        create_output_dir()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        csv_file = os.path.join(OUTPUT_DIR, f"aleo_google_{timestamp}.csv")
        xlsx_file = os.path.join(OUTPUT_DIR, f"aleo_google_{timestamp}.xlsx")
        
        df = pd.DataFrame(all_companies)
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        df.to_excel(xlsx_file, index=False, engine='openpyxl')
        
        print("\n" + "="*60)
        print("‚úÖ HOTOVO!")
        print("="*60)
        print(f"Celkem firem: {len(all_companies)}")
        print(f"Firem s emailem: {sum(1 for c in all_companies if c['email'])}")
        print(f"\nüìÅ Soubory:")
        print(f"  ‚Ä¢ {csv_file}")
        print(f"  ‚Ä¢ {xlsx_file}")
    else:
        print("\n‚ùå Nebyly nalezeny ≈æ√°dn√© firmy!")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Aleo.com scraper s Google vyhled√°v√°n√≠m')
    parser.add_argument('--categories', nargs='+', help='Seznam URL kategori√≠')
    parser.add_argument('--category-file', default='kategorie.txt', help='Soubor s kategoriemi')
    parser.add_argument('--max', type=int, default=10, help='Max firem z ka≈æd√© kategorie')
    
    args = parser.parse_args()
    
    categories = []
    if args.categories:
        categories = args.categories
    elif os.path.exists(args.category_file):
        with open(args.category_file, 'r', encoding='utf-8') as f:
            categories = [line.strip() for line in f if line.strip() and not line.startswith('#')]
    
    if not categories:
        print("‚ùå Chyba: Nebyly zad√°ny kategorie!")
        exit(1)
    
    main(categories, args.max)
